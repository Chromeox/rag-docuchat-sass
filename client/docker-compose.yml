# DocuChat - Full Stack Docker Compose
# Usage: docker-compose up -d

version: '3.8'

services:
  # Frontend - Next.js
  frontend:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - NEXT_PUBLIC_API_URL=http://backend:8000
        - NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=${CLERK_PUBLISHABLE_KEY}
    ports:
      - "3000:3000"
    environment:
      - CLERK_SECRET_KEY=${CLERK_SECRET_KEY}
    depends_on:
      - backend
    restart: unless-stopped

  # Backend - FastAPI (assumes backend is in ../server or ../backend)
  backend:
    build:
      context: ../server
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8000
      - OLLAMA_HOST=ollama
      - OLLAMA_PORT=11434
      - LLM_MODEL=${LLM_MODEL:-llama3.2:3b}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-nomic-embed-text}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    volumes:
      - docuchat-data:/app/data
    depends_on:
      - chromadb
      - ollama
    restart: unless-stopped

  # Vector Database
  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - chroma-data:/chroma/chroma
    restart: unless-stopped

  # Local LLM (optional - comment out if using OpenAI)
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    # Uncomment for GPU support:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    restart: unless-stopped

volumes:
  docuchat-data:
  chroma-data:
  ollama-models:
